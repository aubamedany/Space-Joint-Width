{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.2 üöÄ Python-3.8.0 torch-2.2.1 CPU (Apple M1)\n",
      "Setup complete ‚úÖ (8 CPUs, 16.0 GB RAM, 271.0/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "from torchmetrics import Recall\n",
    "from torchmetrics import Accuracy\n",
    "from preprocess import *\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label={\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images class: 0\n",
      "Processing images class: 1\n",
      "Processing images class: 2\n",
      "Processing images class: 3\n",
      "Processing images class: 4\n",
      "Processing images class: 0\n",
      "Processing images class: 1\n",
      "Processing images class: 2\n",
      "Processing images class: 3\n",
      "Processing images class: 4\n",
      "Processing images class: 0\n",
      "Processing images class: 1\n",
      "Processing images class: 2\n",
      "Processing images class: 3\n",
      "Processing images class: 4\n"
     ]
    }
   ],
   "source": [
    "Xtrain,Ytrain,Xtest,Ytest,Xval,Yval  = Preprocess_GD().pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "class Config:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_epochs = 50\n",
    "    num_classes = 3\n",
    "    learning_rate = 1e-4\n",
    "    model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "    lr_decay = 0.1\n",
    "    patience = 5\n",
    "    batch_size = 32\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config.model\n",
    "model.fc = nn.Linear(model.fc.in_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/namle/Desktop/SegJSW/bestresnet.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(*tensors, **kwargs):\n",
    "\n",
    "    batch_size = kwargs['batch_size']\n",
    "\n",
    "    if len(tensors) == 1:\n",
    "        tensor = tensors[0]\n",
    "        for i in range(0, len(tensor), batch_size):\n",
    "            yield tensor[i:i + batch_size]\n",
    "    else:\n",
    "        for i in range(0, len(tensors[0]), batch_size):\n",
    "            yield tuple(x[i:i + batch_size] for x in tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_preds = torch.tensor([])\n",
    "Yval_preds = torch.tensor([])\n",
    "Ytest_preds = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (minibatch_num,(Xbtrain)) \\\n",
    "                      in enumerate(minibatch(Xtrain,\n",
    "                                                      batch_size=32)):\n",
    "\n",
    "    Ytrain_preds = torch.concat([Ytrain_preds,model(Xbtrain)],dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (minibatch_num,(Xbval)) \\\n",
    "                      in enumerate(minibatch(Xval,\n",
    "                                                      batch_size=32)):\n",
    "    Yval_preds = torch.concat([Yval_preds,model(Xbval)],dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (minibatch_num,(Xbtest)) \\\n",
    "                      in enumerate(minibatch(Xtest,\n",
    "                                                      batch_size=32)):\n",
    "    Ytest_preds = torch.concat([Ytest_preds,model(Xbtest)],dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(Xtrain[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = model(Xtrain[:10])\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.concat([preds,t],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5778, 3, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_process = Preprocess_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è 'hide_labels' is deprecated and will be removed in 'ultralytics 8.399999999999999' in the future. Please use 'show_labels' instead.\n",
      "WARNING ‚ö†Ô∏è 'boxes' is deprecated and will be removed in 'ultralytics 8.399999999999999' in the future. Please use 'show_boxes' instead.\n",
      "\n",
      "WARNING ‚ö†Ô∏è torch.Tensor inputs should be normalized 0.0-1.0 but max value is 252.0. Dividing input by 255.\n",
      "0: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "1: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "2: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "3: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "4: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "5: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "6: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "7: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "8: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "9: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "10: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "11: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "12: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "13: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "14: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "15: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "16: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "17: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "18: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "19: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "20: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "21: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "22: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "23: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "24: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "25: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "26: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "27: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "28: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "29: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "30: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "31: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "32: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "33: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "34: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "35: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "36: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "37: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "38: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "39: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "40: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "41: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "42: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "43: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "44: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "45: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "46: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "47: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "48: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "49: 224x224 1 femur, 1 tibia, 267.6ms\n",
      "Speed: 0.0ms preprocess, 267.6ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "maskch,upper_left,upper_right,lower_left,lower_right = yolo_process.pipeline(Xtrain[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upper_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
